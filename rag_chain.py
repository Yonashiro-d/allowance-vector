# Databricks notebook source
# MAGIC %md
# MAGIC # RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
# MAGIC
# MAGIC Databricks Vector Searchã‚’ä½¿ç”¨ã—ã¦RAGãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

# COMMAND ----------

# MAGIC %pip install -r requirements.txt

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

from typing import Dict, Any
from pyspark.sql import SparkSession
import mlflow

spark = SparkSession.builder.getOrCreate()

# COMMAND ----------

from rag_config import RAGConfig
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from databricks_langchain import ChatDatabricks, DatabricksVectorSearch
from langchain_huggingface import HuggingFaceEmbeddings

config = RAGConfig()
VECTOR_SEARCH_ENDPOINT = "databricks-bge-large-en-endpoint"

print(f"CATALOG: {config.catalog}")
print(f"SCHEMA: {config.schema}")
print(f"VECTOR_INDEX_NAME: {config.vector_index_name}")
print(f"VECTOR_SEARCH_ENDPOINT: {VECTOR_SEARCH_ENDPOINT}")
print(f"QUERY_EMBEDDING_MODEL: {config.query_embedding_model}")
print(f"LLM_ENDPOINT: {config.llm_endpoint}")
print(f"RETRIEVER_TOP_K: {config.retriever_top_k}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰

# COMMAND ----------

chain_config = {
    "llm_model_serving_endpoint_name": config.llm_endpoint,
    "vector_search_endpoint_name": VECTOR_SEARCH_ENDPOINT,
    "vector_search_index": config.vector_index_name,
    "llm_prompt_template": """ã‚ãªãŸã¯è³ªå•ã«ç­”ãˆã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã‚’ã‚‚ã¨ã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚ä¸€éƒ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒç„¡é–¢ä¿‚ãªå ´åˆã€ãã‚Œã‚’å›ç­”ã«åˆ©ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {input}""",
}

print("Chain Config:")
for key, value in chain_config.items():
    print(f"  {key}: {value}")

# COMMAND ----------

def build_rag_chain(chain_config, config):
    """RAGãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰"""
    embedding_model = HuggingFaceEmbeddings(model_name=config.query_embedding_model)
    
    vector_store = DatabricksVectorSearch(
        index_name=chain_config["vector_search_index"],
        embedding=embedding_model,
        text_column="chunked_text",
        columns=["chunk_id", "chunked_text"]
    )
    
    llm = ChatDatabricks(
        endpoint=chain_config["llm_model_serving_endpoint_name"],
        extra_params={"temperature": 0.1}
    )
    
    retriever = vector_store.as_retriever(search_kwargs={"k": config.retriever_top_k})
    prompt = ChatPromptTemplate.from_template(chain_config["llm_prompt_template"])
    document_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, document_chain)
    
    return rag_chain, retriever, vector_store

rag_chain, retriever, vector_store = build_rag_chain(chain_config, config)
print("RAG Chain created successfully")

# VectorStoreRetrieverã®æƒ…å ±ã‚’è¡¨ç¤º
print("\n=== VectorStoreRetriever Information ===")
print(f"Retriever Type: {type(retriever).__name__}")
print(f"Vector Store Type: {type(vector_store).__name__}")
print(f"Index Name: {chain_config['vector_search_index']}")
print(f"Top K: {config.retriever_top_k}")
print(f"Text Column: chunked_text")
print(f"Columns: ['chunk_id', 'chunked_text']")
print("=" * 40)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Build: MLflow Traceè¨˜éŒ²

# COMMAND ----------

# MLflow Trace UIç”¨ã«ãƒã‚§ãƒ¼ãƒ³ã‚’å®Ÿè¡Œã—ã¦ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¨˜éŒ²ï¼ˆBuildï¼‰
with mlflow.start_run(run_name="commuting-allowance-rag-chain"):
    # ãƒã‚§ãƒ¼ãƒ³ã®è¨­å®šã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°
    mlflow.log_params({
        "llm_model_serving_endpoint_name": chain_config["llm_model_serving_endpoint_name"],
        "vector_search_endpoint_name": chain_config["vector_search_endpoint_name"],
        "vector_search_index": chain_config["vector_search_index"],
        "query_embedding_model": config.query_embedding_model,
        "retriever_top_k": config.retriever_top_k,
        "catalog": config.catalog,
        "schema": config.schema
    })
    
    mlflow.set_tag("task", "llm/v1/chat")
    mlflow.set_tag("embedding_model", config.query_embedding_model)
    mlflow.set_tag("llm", chain_config["llm_model_serving_endpoint_name"])
    mlflow.set_tag("model_type", "chat_completion")
    mlflow.set_tag("chain_type", "retrieval_chain")
    
    # ãƒã‚§ãƒ¼ãƒ³è¨­å®šã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜
    mlflow.log_dict(chain_config, "chain_config.json")
    
    # MLflow Trace UIç”¨ã«ãƒã‚§ãƒ¼ãƒ³ã‚’å®Ÿè¡Œï¼ˆMLflow 2.14.0+ã§ã¯è‡ªå‹•çš„ã«ãƒˆãƒ¬ãƒ¼ã‚¹ãŒè¨˜éŒ²ã•ã‚Œã‚‹ï¼‰
    print("Executing RAG chain for MLflow Trace UI...")
    
    # MLflow LangChain autologã‚’æœ‰åŠ¹åŒ–ï¼ˆMLflow 2.14.0+ã®å ´åˆï¼‰
    try:
        mlflow.langchain.autolog()
    except AttributeError:
        print("Note: mlflow.langchain.autolog() is not available in this MLflow version.")
        print("Traces will still be recorded when the chain is invoked.")
    
    # ãƒã‚§ãƒ¼ãƒ³ã‚’å®Ÿè¡Œã—ã¦ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¨˜éŒ²ï¼ˆBuildã®ä¸€éƒ¨ï¼‰
    trace_question = "é€šå‹¤æ‰‹å½“ã¯ã„ãã‚‰ã¾ã§æ”¯çµ¦ã•ã‚Œã¾ã™ã‹ï¼Ÿ"
    trace_result = rag_chain.invoke({"input": trace_question})
    
    # ãƒˆãƒ¬ãƒ¼ã‚¹çµæœã‚’ãƒ­ã‚°
    context_docs = trace_result.get("context", [])
    mlflow.log_dict({
        "question": trace_question,
        "answer": trace_result.get("answer", ""),
        "context_documents_count": len(context_docs)
    }, "chain_trace_result.json")
    
    print("âœ… MLflow Trace recorded successfully")
    print(f"Run ID: {mlflow.active_run().info.run_id}")
    print("ğŸ’¡ You can view the trace in MLflow UI under the 'Traces' tab")

# COMMAND ----------
# Unity Catalogã«æ¥ç¶š
mlflow.set_registry_uri("databricks-uc")

# Unity Catalogãƒ¢ãƒ‡ãƒ«å
UC_MODEL_NAME = f"{config.catalog}.{config.schema}.commuting_allowance_rag_agent"

# COMMAND ----------

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’MLflowã«ãƒ­ã‚°
# mlflow.pyfunc.log_modelã‚’ä½¿ç”¨
from mlflow.models.resources import DatabricksServingEndpoint

# ãƒªã‚½ãƒ¼ã‚¹å®šç¾©ï¼ˆLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼‰
resources = [
    DatabricksServingEndpoint(endpoint_name=chain_config["llm_model_serving_endpoint_name"])
]

with mlflow.start_run(run_name="commuting-allowance-rag-agent"):
    with open("requirements.txt", "r") as f:
        pip_requirements = [line.strip() for line in f if line.strip() and not line.startswith("#")]
    
    logged_model_info = mlflow.pyfunc.log_model(
        name="agent",
        python_model="agent.py",
        pip_requirements=pip_requirements,
        resources=resources,
    )
    
    print(f"âœ… Agent logged: {logged_model_info.model_uri}")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã‚¿ã‚°ã‚’ãƒ­ã‚°
    mlflow.log_params({
        "llm_model_serving_endpoint_name": chain_config["llm_model_serving_endpoint_name"],
        "vector_search_endpoint_name": chain_config["vector_search_endpoint_name"],
        "vector_search_index": chain_config["vector_search_index"],
        "query_embedding_model": config.query_embedding_model,
        "retriever_top_k": config.retriever_top_k,
        "catalog": config.catalog,
        "schema": config.schema
    })
    
    mlflow.set_tag("task", "llm/v1/chat")
    mlflow.set_tag("embedding_model", config.query_embedding_model)
    mlflow.set_tag("llm", chain_config["llm_model_serving_endpoint_name"])
    mlflow.set_tag("model_type", "databricks-agent")
    mlflow.set_tag("chain_type", "retrieval_chain")
    
    print(f"Run ID: {mlflow.active_run().info.run_id}")

# COMMAND ----------

# Unity Catalogã«ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²
uc_registered_model_info = mlflow.register_model(
    model_uri=logged_model_info.model_uri,
    name=UC_MODEL_NAME
)

print(f"âœ… Model registered to Unity Catalog: {UC_MODEL_NAME}")
print(f"   Version: {uc_registered_model_info.version}")

# COMMAND ----------

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã«ãƒ‡ãƒ—ãƒ­ã‚¤
from databricks import agents
from databricks.sdk import WorkspaceClient

print(f"Deploying agent to serving endpoint...")
print(f"  Model: {UC_MODEL_NAME}")
print(f"  Version: {uc_registered_model_info.version}")
print(f"  Endpoint: commuting-allowance-rag-agent-endpoint")

try:
    deployment_info = agents.deploy(
        model_name=UC_MODEL_NAME,
        model_version=uc_registered_model_info.version,
        endpoint_name="commuting-allowance-rag-agent-endpoint"
    )
    
    print(f"âœ… Agent deployed successfully!")
    print(f"   Deployment info: {deployment_info}")
    
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
    w = WorkspaceClient()
    endpoint_name = "commuting-allowance-rag-agent-endpoint"
    
    try:
        endpoint = w.serving_endpoints.get(endpoint_name)
        endpoint_url = f"{w.config.host}/serving-endpoints/{endpoint_name}/invocations"
        
        print(f"\n=== Endpoint Information ===")
        print(f"Endpoint Name: {endpoint_name}")
        print(f"Endpoint URL: {endpoint_url}")
        print(f"Endpoint State: {endpoint.state}")
        print(f"\nğŸ’¡ You can now use the agent in Databricks Playground!")
        print(f"ğŸ’¡ Review App and API endpoint are available")
        print(f"\nğŸ“ Test the agent:")
        print(f"   from databricks.sdk import WorkspaceClient")
        print(f"   w = WorkspaceClient()")
        print(f"   client = w.serving_endpoints.get_open_ai_client()")
        print(f"   response = client.chat.completions.create(")
        print(f"       model=\"{endpoint_name}\",")
        print(f"       messages=[{{\"role\": \"user\", \"content\": \"é€šå‹¤æ‰‹å½“ã¯ã„ãã‚‰ã¾ã§æ”¯çµ¦ã•ã‚Œã¾ã™ã‹ï¼Ÿ\"}}]")
        print(f"   )")
        print(f"   print(response.choices[0].message.content)")
    except Exception as e:
        print(f"âš ï¸ Could not retrieve endpoint information: {e}")
        print(f"ğŸ’¡ Please check the Databricks UI for the endpoint details")
        
except Exception as e:
    print(f"âŒ Error deploying agent: {e}")
    print(f"\nTroubleshooting steps:")
    print(f"1. Check if the model is correctly registered in Unity Catalog")
    print(f"2. Verify model version: {uc_registered_model_info.version}")
    print(f"3. Check if endpoint name 'commuting-allowance-rag-agent-endpoint' is available")
    print(f"4. Verify permissions for model serving")
    raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ
# MAGIC
# MAGIC ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

# COMMAND ----------

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒæº–å‚™ã§ãã‚‹ã¾ã§å°‘ã—å¾…ã¤å¿…è¦ãŒã‚ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™
import time

endpoint_name = "commuting-allowance-rag-agent-endpoint"
print(f"Testing agent endpoint: {endpoint_name}")

try:
    w = WorkspaceClient()
    
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®çŠ¶æ…‹ã‚’ç¢ºèª
    endpoint = w.serving_endpoints.get(endpoint_name)
    print(f"Endpoint state: {endpoint.state}")
    
    if endpoint.state.get("ready") == "READY":
        # OpenAIäº’æ›ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—
        client = w.serving_endpoints.get_open_ai_client()
        
        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
        test_messages = [
            {"role": "user", "content": "é€šå‹¤æ‰‹å½“ã¯ã„ãã‚‰ã¾ã§æ”¯çµ¦ã•ã‚Œã¾ã™ã‹ï¼Ÿ"}
        ]
        
        print(f"\nSending test query: {test_messages[0]['content']}")
        response = client.chat.completions.create(
            model=endpoint_name,
            messages=test_messages
        )
        
        print(f"\nâœ… Agent response:")
        print(f"   {response.choices[0].message.content}")
    else:
        print(f"âš ï¸ Endpoint is not ready yet. State: {endpoint.state}")
        print(f"ğŸ’¡ Please wait a few minutes and test manually in Databricks Playground")
        
except Exception as e:
    print(f"âš ï¸ Could not test endpoint automatically: {e}")
    print(f"ğŸ’¡ You can test the agent manually:")
    print(f"   1. Go to Databricks UI > Serving > {endpoint_name}")
    print(f"   2. Click 'Open in Playground'")
    print(f"   3. Send a test message: 'é€šå‹¤æ‰‹å½“ã¯ã„ãã‚‰ã¾ã§æ”¯çµ¦ã•ã‚Œã¾ã™ã‹ï¼Ÿ'")